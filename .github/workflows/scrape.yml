name: Wind Data Scraper

on:
  schedule:
    # Runs at 00:00 UTC every day
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      # Install Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      # Install Chromium and dependencies
      - name: Install Chromium and dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser
      
      # Install Python dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium webdriver-manager pandas firebase-admin
      
      # Add service account key (you'll need to set this as a secret in your repo settings)
      - name: Add Firebase service account key
        run: |
          mkdir -p ${{ github.workspace }}/.github/workflows/
          echo "${{ secrets.FIREBASE_SERVICE_ACCOUNT_KEY }}" > serviceAccountKey.json
      
      # Run the scraper
      - name: Run scraper
        run: python scraper.py
      
      # Optional: Commit and push results if you want to save CSV files
      # - name: Commit and push if content changed
      #   run: |-
      #     git config user.name "Automated"
      #     git config user.email "actions@users.noreply.github.com"
      #     git add -A
      #     timestamp=$(date -u)
      #     git commit -m "Latest data: ${timestamp}" || exit 0
      #     git push